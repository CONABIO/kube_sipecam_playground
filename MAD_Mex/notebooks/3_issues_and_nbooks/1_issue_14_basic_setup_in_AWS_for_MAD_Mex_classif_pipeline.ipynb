{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will solve:\n",
    "\n",
    "https://github.com/CONABIO/kube_sipecam_playground/issues/14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up minikube and usage of docker image for MAD-Mex + kale in AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will follow: \n",
    "\n",
    "* For minikube: [minikube_sipecam/setup](https://github.com/CONABIO/kube_sipecam/tree/master/minikube_sipecam/setup#aws)\n",
    "\n",
    "* docker image for MAD-Mex: [kube_sipecam/dockerfiles/MAD_Mex/odc_kale](https://github.com/CONABIO/kube_sipecam/tree/master/dockerfiles/MAD_Mex/odc_kale) and [minikube_sipecam/deployments/MAD_Mex](https://github.com/CONABIO/kube_sipecam/tree/master/minikube_sipecam/deployments/MAD_Mex/)\n",
    "\n",
    "* Reference for this nbook: \n",
    "\n",
    "[1_issue_5_basic_setup_in_AWS_for_MAD_Mex_classif_pipeline](https://github.com/CONABIO/kube_sipecam_playground/blob/master/MAD_Mex/notebooks/1_issue_5_basic_setup_in_AWS_for_MAD_Mex_classif_pipeline.ipynb)\n",
    "\n",
    "[1_issue_10_basic_setup_in_AWS_for_MAD_Mex_classif_pipeline](https://github.com/CONABIO/kube_sipecam_playground/blob/master/MAD_Mex/notebooks/2_issues_and_nbooks/1_issue_10_basic_setup_in_AWS_for_MAD_Mex_classif_pipeline.ipynb.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will use [minikube_sipecam/deployments/MAD_Mex/hostpath_pv](https://github.com/CONABIO/kube_sipecam/tree/master/minikube_sipecam/deployments/MAD_Mex/hostpath_pv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance\n",
    "\n",
    "In AWS account we can select ami: `minikube-sipecam` which has next description:\n",
    "\n",
    "*Based in k8s-1.16-debian-buster-amd64-hvm-ebs-2020-04-27 - ami-0ab39819e336a3f3f Contains kubectl 1.19.1 minikube 1.13.0 kubeflow 1.0.2*\n",
    "\n",
    "and instance `m5.2xlarge` with `100` gb of disk.\n",
    "\n",
    "Use next bash script for user data:\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "##variables:\n",
    "region=us-west-2\n",
    "name_instance=minikube-10-09-2020\n",
    "##System update\n",
    "apt-get update -yq\n",
    "##Tag instance\n",
    "INSTANCE_ID=$(curl -s http://instance-data/latest/meta-data/instance-id)\n",
    "PUBLIC_IP=$(curl -s http://instance-data/latest/meta-data/public-ipv4)\n",
    "aws ec2 create-tags --resources $INSTANCE_ID --tag Key=Name,Value=$name_instance-$PUBLIC_IP --region=$region\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ssh to instance, all commands will be executed as root**\n",
    "\n",
    "`sudo su`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next will start minikube and kubeflow pods:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd /root && minikube start --driver=none\n",
    "\n",
    "cd /opt/kf-test && /root/kfctl apply -V -f kfctl_k8s_istio.v1.0.2.yaml\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check pods and status with:\n",
    "\n",
    "```\n",
    "minikube status\n",
    "\n",
    "minikube\n",
    "type: Control Plane\n",
    "host: Running\n",
    "kubelet: Running\n",
    "apiserver: Running\n",
    "kubeconfig: Configured\n",
    "```\n",
    "\n",
    "```\n",
    "kubectl get pods -n kubeflow\n",
    "\n",
    "#all running except:\n",
    "spark-operatorcrd-cleanup-2p7x2                                0/2     Completed   0          7m6s\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To access kubeflow UI set:**\n",
    "\n",
    "```\n",
    "export INGRESS_HOST=$(minikube ip)\n",
    "export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\"http2\")].nodePort}')\n",
    "echo $INGRESS_PORT\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And go to:**\n",
    "\n",
    "```\n",
    "http://<ipv4 of ec2 instance>:$INGRESS_PORT\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployments and services \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "MAD_MEX_LOAD_BALANCER_SERVICE=loadbalancer-mad-mex-0.1.0_1.7.0_0.5.0-hostpath-pv\n",
    "MAD_MEX_PV=hostpath-pv\n",
    "MAD_MEX_PVC=hostpath-pvc\n",
    "MAD_MEX_JUPYTERLAB_SERVICE=jupyterlab-mad-mex-0.1.0_1.7.0_0.5.0-hostpath-pv\n",
    "MAD_MEX_URL=https://raw.githubusercontent.com/CONABIO/kube_sipecam/master/minikube_sipecam/deployments/MAD_Mex/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create storage:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "kubectl create -f $MAD_MEX_URL/hostpath_pv/$MAD_MEX_PV.yaml\n",
    "kubectl create -f $MAD_MEX_URL/hostpath_pv/$MAD_MEX_PVC.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create service:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "kubectl create -f $MAD_MEX_URL/hostpath_pv/$MAD_MEX_LOAD_BALANCER_SERVICE.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create deployment:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "kubectl create -f $MAD_MEX_URL/hostpath_pv/$MAD_MEX_JUPYTERLAB_SERVICE.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And go to:**\n",
    "\n",
    "```\n",
    "http://<ipv4 of ec2 instance>:30001/madmexurl\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up postgresql instance in AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will follow:\n",
    "\n",
    "https://github.com/CONABIO/antares3-docker/tree/master/postgresql/local_deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clone, init DB**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd /shared_volume\n",
    "dir=/shared_volume/postgresql_volume_docker\n",
    "mkdir $dir\n",
    "\n",
    "git clone https://github.com/CONABIO/antares3-docker.git $dir/antares3-docker\n",
    "\n",
    "mkdir -p $dir/etc/postgresql\n",
    "mkdir -p $dir/var/log/postgresql\n",
    "mkdir -p $dir/var/lib/postgresql\n",
    "\n",
    "docker run -v $dir/etc/postgresql:/etc/postgresql \\\n",
    "-v $dir/var/log/postgresql:/var/log/postgresql \\\n",
    "-v $dir/var/lib/postgresql:/var/lib/postgresql \\\n",
    "-v $dir/antares3-docker/postgresql/local_deployment/conf/:/home/postgres/conf/ \\\n",
    "-w /home/postgres \\\n",
    "-p 2225:22 -p 2345:5432 --name postgresql-madmex-odc --hostname postgresql-madmex \\\n",
    "-dit madmex/postgresql-madmex-local:v8 /bin/bash\n",
    "\n",
    "docker exec -it postgresql-madmex-odc /usr/local/bin/entrypoint.sh\n",
    "docker exec -u=postgres -it postgresql-madmex-odc /home/postgres/conf/setup.sh\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create `/shared_volume/.geonode_conabio`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "HOST_NAME=\"<ipv4 DNS of ec2>\"\n",
    "USER_GEOSERVER=\"super\"\n",
    "PASSWORD_GEOSERVER=\"duper\"\n",
    "PASSWORD_DB_GEONODE_DATA=\"geonode\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init files for antares3 and ODC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next commands in jupyterlab**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`~/.datacube.conf`\n",
    "\n",
    "```\n",
    "[user]\n",
    "default_environment: datacube\n",
    "#default_environment: s3aio_env\n",
    "\n",
    "[datacube]\n",
    "db_hostname: 172.17.0.1\n",
    "db_database: antares_datacube\n",
    "db_username: postgres\n",
    "db_password: postgres\n",
    "db_port: 2345\n",
    "\n",
    "\n",
    "execution_engine.use_s3: False\n",
    "\n",
    "[s3aio_env]\n",
    "db_hostname: 172.17.0.1\n",
    "db_database: antares_datacube\n",
    "db_username: postgres\n",
    "db_password: postgres\n",
    "db_port: 2345\n",
    "\n",
    "#index_driver: s3aio_index\n",
    "\n",
    "execution_engine.use_s3: False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`~/.antares`\n",
    "\n",
    "```\n",
    "# Django settings\n",
    "SECRET_KEY=<key>\n",
    "DEBUG=True\n",
    "DJANGO_LOG_LEVEL=DEBUG\n",
    "ALLOWED_HOSTS=\n",
    "# Database\n",
    "DATABASE_NAME=antares_datacube\n",
    "DATABASE_USER=postgres\n",
    "DATABASE_PASSWORD=postgres\n",
    "DATABASE_HOST=172.17.0.1\n",
    "DATABASE_PORT=2345\n",
    "# Datacube\n",
    "SERIALIZED_OBJECTS_DIR=/shared_volume/datacube_ingest/serialized_objects/\n",
    "INGESTION_PATH=/shared_volume/datacube_ingest\n",
    "#DRIVER=s3aio\n",
    "DRIVER='NetCDF CF'\n",
    "#INGESTION_BUCKET=datacube-s2-jalisco-test\n",
    "# Query and download\n",
    "USGS_USER=<username>\n",
    "USGS_PASSWORD=<password>\n",
    "SCIHUB_USER=\n",
    "SCIHUB_PASSWORD=\n",
    "# Misc\n",
    "BIS_LICENSE=<license>\n",
    "TEMP_DIR=/shared_volume/temp\n",
    "SEGMENTATION_DIR=/shared_volume/segmentation/\n",
    "#SEGMENTATION_BUCKET=<name of bucket>\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create dir for segmentation if will hold results of that process:**\n",
    "\n",
    "`mkdir /shared_volume/segmentation/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upgrade antares with no deps:**\n",
    "\n",
    "`pip3 install --user git+https://github.com/CONABIO/antares3.git@develop --upgrade --no-deps`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Init antares and datacube:**\n",
    "\n",
    "```\n",
    "~/.local/bin/antares init\n",
    "datacube -v system init\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check:**\n",
    "\n",
    "`datacube -v system check`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create spatial indexes:**\n",
    "\n",
    "```\n",
    "apt-get install -y postgresql-client\n",
    "psql -h 172.17.0.1 -d antares_datacube -U postgres -p 2345\n",
    "#password postgres\n",
    "CREATE INDEX madmex_predictobject_gix ON public.madmex_predictobject USING GIST (the_geom);\n",
    "CREATE INDEX madmex_trainobject_gix ON public.madmex_trainobject USING GIST (the_geom);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are some notes that could be followed [Notes](https://github.com/CONABIO/antares3-docker/tree/master/postgresql/local_deployment#note) for docker container of postgresql**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register and ingest LANDSAT 8 data into ODC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S3 bucket that has data: `landsat-images-kube-sipecam-mad-mex`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare metadata:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "~/.local/bin/antares prepare_metadata --path \"/\" --bucket landsat-images-kube-sipecam-mad-mex --dataset_name landsat_espa --outfile /shared_volume/metadata_mex_l8.yaml --multi 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datacube ingestion:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "datacube -v product add ~/.config/madmex/indexing/ls8_espa_scenes.yaml\n",
    "datacube -v dataset add /shared_volume/metadata_mex_l8.yaml\n",
    "datacube -v ingest -c ~/.config/madmex/ingestion/ls8_espa_mexico.yaml --executor multiproc 6\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register and ingest SRTM data into ODC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using https://conabio.github.io/antares3/example_s2_land_cover.html#prepare-terrain-metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From http://dwtkns.com/srtm/ will download srtm data for Chiapas:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd /shared_volume\n",
    "wget http://srtm.csi.cgiar.org/wp-content/uploads/files/srtm_5x5/tiff/srtm_18_09.zip\n",
    "apt-get install -y unzip\n",
    "unzip srtm_18_09.zip -d /shared_volume/srtm_18_09\n",
    "mkdir /shared_volume/srtm_mosaic\n",
    "cp /shared_volume/srtm_18_09/srtm_18_09.tif /shared_volume/srtm_mosaic/srtm_mosaic.tif\n",
    "gdaldem slope /shared_volume/srtm_mosaic/srtm_mosaic.tif /shared_volume/srtm_mosaic/slope_mosaic.tif -s 111120\n",
    "gdaldem aspect /shared_volume/srtm_mosaic/srtm_mosaic.tif /shared_volume/srtm_mosaic/aspect_mosaic.tif\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create product and Index mosaic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`datacube -v product add ~/.config/madmex/indexing/srtm_cgiar.yaml`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "~/.local/bin/antares prepare_metadata --path /shared_volume/srtm_mosaic --dataset_name srtm_cgiar --outfile /shared_volume/metadata_srtm.yaml\n",
    "\n",
    "datacube -v dataset add /shared_volume/metadata_srtm.yaml\n",
    "datacube -v ingest -c ~/.config/madmex/ingestion/srtm_cgiar_mexico.yaml --executor multiproc 6\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest Mexico's shapefile to antares-datacube DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`~/.local/bin/antares init -c 'MEX'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest training data in antares-datacube DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training data is in bucket `training-data-kube-sipecam-mad-mex`**\n",
    "\n",
    "```\n",
    "Chiapas_31.shp\n",
    "Chiapas_31.shx\n",
    "Chiapas_31.prj\n",
    "Chiapas_31.dbf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "~/.local/bin/antares ingest_training_from_vector /shared_volume/training_data/Chiapas_31.shp --scheme madmex --year 2015 --name train_chiapas_dummy --field class\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy geonode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Being in EC2 instance as root**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following: https://github.com/CONABIO/geonode/tree/master/deployment_using_spcgeonode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being root `sudo su`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install docker-compose:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd ~\n",
    "curl -L \"https://github.com/docker/compose/releases/download/1.26.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\n",
    "chmod +x /usr/local/bin/docker-compose\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy geonode using https://github.com/CONABIO/geonode/tree/master/deployment_using_spcgeonode instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When cloning repo of geonode in `/shared_volume` change `/shared_volume/geonode/scripts/spcgeonode/nginx/nginx.conf.envsubst` to `server_names_hash_bucket_size  128;` and use in `/shared_volume/geonode/scripts/spcgeonode/.env` `ipv4 dns of ec2 instance`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And add rule in security groups for `80` port"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployments and services \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "GEONODE_CONABIO_LOAD_BALANCER_SERVICE=loadbalancer-geonode-conabio-0.1_0.5.0-hostpath-pv\n",
    "GEONODE_CONABIO_PV=hostpath-pv\n",
    "GEONODE_CONABIO_PVC=hostpath-pvc\n",
    "GEONODE_CONABIO_JUPYTERLAB_SERVICE_HOSTPATH_PV=jupyterlab-geonode-conabio-0.1_0.5.0-hostpath-pv\n",
    "GEONODE_CONABIO_URL=https://raw.githubusercontent.com/CONABIO/kube_sipecam/master/minikube_sipecam/deployments/geonode_conabio/\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create storage:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "kubectl create -f $GEONODE_CONABIO_URL/hostpath_pv/$GEONODE_CONABIO_PV.yaml\n",
    "kubectl create -f $GEONODE_CONABIO_URL/hostpath_pv/$GEONODE_CONABIO_PVC.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create service:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "kubectl create -f $GEONODE_CONABIO_URL/hostpath_pv/$GEONODE_CONABIO_LOAD_BALANCER_SERVICE.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create deployment:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "kubectl create -f $GEONODE_CONABIO_URL/hostpath_pv/$GEONODE_CONABIO_JUPYTERLAB_SERVICE_HOSTPATH_PV.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And go to:**\n",
    "\n",
    "```\n",
    "http://<ipv4 of ec2 instance>:30002/geonodeurl\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:\n",
    "\n",
    "If disk is full which could happen if a kubeflow pipeline will be uploaded from kale:\n",
    "\n",
    "```\n",
    "HTTP response headers: HTTPHeaderDict({'Date': 'Tue, 01 Sep 2020 18:12:22 GMT', 'Content-Length': '487', 'Content-Type': 'text/plain; charset=utf-8'})\n",
    "HTTP response body: {\"error_message\":\"Error creating pipeline: Create pipeline failed: InternalServerError: Failed to store b2fa5a70-cab4-4c89-8784-9c0cb118d1b4: Storage backend has reached its minimum free disk threshold. Please delete a few objects to proceed.\",\"error_details\":\"Error creating pipeline: Create pipeline failed: InternalServerError: Failed to store b2fa5a70-cab4-4c89-8784-9c0cb118d1b4: Storage backend has reached its minimum free disk threshold. Please delete a few objects to proceed.\"}\n",
    "```\n",
    "\n",
    "Delete kubeflow (MAD-Mex and geonode deployments)\n",
    "\n",
    "To free space:\n",
    "\n",
    "```\n",
    "minikube stop\n",
    "minikube delete\n",
    "```\n",
    "\n",
    "Check:\n",
    "\n",
    "```\n",
    "docker system df\n",
    "docker system prune --all --volumes\n",
    "rm -r /root/.minikube/*\n",
    "rm -r /root/.kube/*\n",
    "rm -r /opt/kf-test\n",
    "```\n",
    "\n",
    "Start again (being in root dir):\n",
    "\n",
    "```\n",
    "CONFIG_URI=\"https://raw.githubusercontent.com/kubeflow/manifests/v1.0-branch/kfdef/kfctl_k8s_istio.v1.0.2.yaml\"\n",
    "source ~/.profile\n",
    "chmod gou+wrx -R /opt/\n",
    "mkdir -p ${KF_DIR}\n",
    "#minikube start\n",
    "cd /root && minikube start --driver=none\n",
    "#kubeflow start\n",
    "cd ${KF_DIR}\n",
    "\n",
    "wget $CONFIG_URI\n",
    "wget https://codeload.github.com/kubeflow/manifests/tar.gz/v1.0.2 -O v1.0.2.tar.gz\n",
    "\n",
    "```\n",
    "\n",
    "change kfctl_k8s_istio.v1.0.2.yaml at the end uri:\n",
    "\n",
    "```\n",
    "#this section:\n",
    "  repos:\n",
    "  - name: manifests\n",
    "    uri: https://github.com/kubeflow/manifests/archive/v1.0.2.tar.gz\n",
    "#for: \n",
    "  repos:\n",
    "  - name: manifests\n",
    "    uri: file:///opt/kf-test/v1.0.2.tar.gz\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "```\n",
    "kfctl apply -V -f kfctl_k8s_istio.v1.0.2.yaml\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "ref: https://github.com/aws-samples/eks-workshop/issues/639\n",
    "\n",
    "If there's problems with geonode (because stack of docker-compose was deleted, clone again repo and deploy geonode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "docker_image": "",
   "experiment": {
    "id": "",
    "name": ""
   },
   "experiment_name": "",
   "pipeline_description": "",
   "pipeline_name": "",
   "volumes": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
